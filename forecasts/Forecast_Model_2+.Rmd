---
title: "Forecast_Additional_Models"
author: "Justin Maynard"
date: "2025-03-31"
output: pdf_document
always_allow_html: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

## Setting R code chunk options

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

## Loading packages and initializing

```{r package, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
library(forecast)
library(openxlsx)
library(here)
library(tseries)
library(tidyr)
```

## Import and transform load data

```{r}
#Import hourly load data
load_data <- read_excel("data/load.xlsx")

#Convert to daily average load
load_daily <- load_data %>%
  mutate(DailyLoad = rowMeans(select(., h1:h24), na.rm = TRUE)) %>%
  select(date, DailyLoad)

summary(load_daily)

#Transforming into time series objects
load_daily_ts <- msts(load_daily$DailyLoad,
                start = c(2005, 1, 1),
                seasonal.periods = c(7, 365.25))

#Plot the time series
autoplot(load_daily_ts) +
  labs(title = "Daily Electricity Load Time Series")

#Decompose daily load time series
load_daily_ts %>% mstl() %>%
  autoplot()

```

## Prepare training and test sets

```{r}
#Create a subset for training purpose from 2005-01-01 to 2009-12-31
n_for = 59

load_daily_train_ts <- subset(load_daily_ts, end = length(load_daily_ts) - n_for)

#Create a subset for testing purpose from 2010-01-01 to 2010-02-28
load_daily_test_ts   <- subset(load_daily_ts, start = length(load_daily_ts) - n_for + 1)


#Plot training and testing sets
autoplot(load_daily_train_ts)
autoplot(load_daily_test_ts)
```

## Modles:
SARIMA, ARIMA + Fourier, STL + ETS, BSM, TBATS, NNETAR

## Model 1 - Naive

```{r}
#Fit naive model to training data
naive_model <- naive(load_daily_train_ts, h = n_for)

#Plot forecast vs actual
autoplot(naive_model) +
  autolayer(load_daily_test_ts, series = "Actual", color = "red") +
  labs(title = "Naive Forecast vs Actual 2010 Janâ€“Feb",
       y = "Daily Load")

#Refit model on full dataset
naive_model_full <- naive(load_daily_ts, h = n_for)
str(naive_model_full)

#Extract forecast values
forecast_2011_model_1 <- as.numeric(naive_model_full$mean)

```

## Model 2 - STL + ETS

```{r}

ETS_fit <- stlf(load_daily_train_ts, h = n_for)

autoplot(ETS_fit) + ylab("Daily Demand")

#plot with full data
autoplot(load_daily_ts) + 
  autolayer(ETS_fit, series = "STL + ETS", PI = FALSE)

#fit on full dataset
ETS_full <- stlf(load_daily_ts, h = n_for)
str(ETS_full)

#accuracy
ETS_scores <- accuracy(ETS_fit$mean, load_daily_test_ts)

forecast_2011_model_2 <- as.numeric(ETS_full$mean)

#Read submission template
template <- read_excel("data/submission_template.xlsx")

#Insert forecast
template$load <- forecast_2011_model_2

#Save as CSV
write.csv(template, "forecasts/forecast_submission_model_2.csv", row.names = FALSE)

```

## Model 3 - ARIMA + FOURIER terms

```{r}

k = c(2,6)

ARIMA_Fourier_Fit <- auto.arima(load_daily_train_ts,
                                seasonal = FALSE,
                                lambda = 0,
                                xreg = fourier(load_daily_train_ts,
                                               K = k))

ARIMA_Fourier_Forecast <- forecast(ARIMA_Fourier_Fit,
                                   xreg = fourier(load_daily_train_ts,
                                                  K = k,
                                                  h = n_for),
                                   h = n_for)

autoplot(ARIMA_Fourier_Forecast) 

autoplot(load_daily_ts) + 
  autolayer(ARIMA_Fourier_Forecast, series = "ARIMA_FOURIER", PI = FALSE)


```

```{r}
#Fit on full dataset

ARIMA_Fourier_Fit_Full <- auto.arima(load_daily_ts,
                                seasonal = FALSE,
                                lambda = 0,
                                xreg = fourier(load_daily_ts,
                                               K = k))

ARIMA_Fourier_Forecast_Full <- forecast(ARIMA_Fourier_Fit_Full,
                                   xreg = fourier(load_daily_ts,
                                                  K = k,
                                                  h = n_for),
                                   h = n_for)

str(ARIMA_Fourier_Forecast_Full)

forecast_2011_model_3 <- as.numeric(ARIMA_Fourier_Forecast_Full$mean)

#Read submission template
template <- read_excel("data/submission_template.xlsx")

#Insert forecast
template$load <- forecast_2011_model_3

#accuracy
ARIMA_Fourier_scores <- accuracy(ARIMA_Fourier_Forecast$mean, load_daily_test_ts)

#Save as CSV
write.csv(template, "forecasts/forecast_submission_model_3.csv", row.names = FALSE)

```




## Model 4 - SARIMA


## Model 5 - TBATS

```{r}

#Fit on training data
TBATS_fit <- tbats(load_daily_train_ts)

TBATS_forecast <- forecast(TBATS_fit, h = n_for)


autoplot(TBATS_forecast) 

autoplot(load_daily_ts) + 
  autolayer(TBATS_forecast, series = "TBATS", PI = FALSE)

TBATS_scores <- accuracy(TBATS_forecast$mean, load_daily_test_ts)


#Fit on full data
#TBATS_full <- tbats(load_daily_ts)
#TBATS_forecast_full <- forecast(TBATS_full, h = n_for)

#forecast_2011_model_5 <- as.numeric(TBATS_forecast_full$mean)

#Read submission template
#template <- read_excel("data/submission_template.xlsx")

#Insert forecast
#template$load <- forecast_2011_model_5

#Save as CSV
#write.csv(template, "forecasts/forecast_submission_model_5.csv", row.names = FALSE)

```


## Model 6 - BSM

```{r}
#variances for level, trend, seasonal, observation

BSM_fit <- StructTS(load_daily_test_ts, type = "BSM", fixed=c(0.1,0.01,0.3,NA))

checkresiduals(BSM_fit)

BSM_forecast <- forecast(BSM_fit, h = n_for)
autoplot(BSM_forecast) 

autoplot(load_daily_ts) + 
  autolayer(BSM_forecast, series = "BSM", PI = FALSE)

```


## Model 7 - NNETAR

```{r}

NN_fit <- nnetar(load_daily_train_ts,
                 p = 1,
                 P = 0,
                 xreg = fourier(load_daily_train_ts, K=c(2,12)))

NN_forecast <- forecast(NN_fit, h = n_for, 
                 xreg = fourier(load_daily_train_ts, 
                                K = c(2,12),
                                h = n_for))
checkresiduals(NN_forecast)



autoplot(NN_forecast)

autoplot(load_daily_ts) + 
  autolayer(NN_forecast, series = "NNETAR", PI = FALSE)

NN_scores <- accuracy(NN_forecast$mean, load_daily_test_ts)

```


```{r}



#Fit on full data
NN_fit_full <- nnetar(load_daily_ts,
                 p = 1,
                 P = 0,
                 xreg = fourier(load_daily_ts, K=c(2,12)))

NN_forecast_full <- forecast(NN_fit_full, h = n_for, 
                 xreg = fourier(load_daily_ts, 
                                K = c(2,12),
                                h = n_for))

forecast_2011_model_7 <- as.numeric(NN_forecast_full$mean)
#Read submission template
template <- read_excel("data/submission_template.xlsx")

#Insert forecast
template$load <- forecast_2011_model_7

#Save as CSV
write.csv(template, "forecasts/forecast_submission_model_7.csv", row.names = FALSE)
```



```{r}
grid_week <- 1:3
grid_year <- 4:15
results <- data.frame()


for (K_w in grid_week) {
  for (K_y in grid_year) {
    K_vec  <- c(K_w, K_y)
    fit    <- nnetar(load_daily_train_ts, p = 1, P = 0,
                     xreg = fourier(load_daily_train_ts, K = K_vec))
    fc     <- forecast(fit, h = n_for,
                       xreg = fourier(load_daily_train_ts, K = K_vec, h = n_for))
    rmse   <- accuracy(fc, load_daily_test_ts)["Test set", "RMSE"]
    results<- rbind(results,
                    data.frame(K_week = K_w, K_year = K_y, RMSE = rmse))
  }
}

best_K <- results[which.min(results$RMSE), c("K_week","K_year")]
print(best_K)

```


```{r}


#Fit on full data
NN_fit_full <- nnetar(load_daily_ts,
                 p = 1,
                 P = 0,
                 xreg = fourier(load_daily_ts, K=c(3,15)))

NN_forecast_full <- forecast(NN_fit_full, h = n_for, 
                 xreg = fourier(load_daily_ts, 
                                K = c(3,15),
                                h = n_for))

forecast_2011_model_8 <- as.numeric(NN_forecast_full$mean)
#Read submission template
template <- read_excel("data/submission_template.xlsx")

#Insert forecast
template$load <- forecast_2011_model_8

#Save as CSV
write.csv(template, "forecasts/forecast_submission_model_8.csv", row.names = FALSE)
```


## Scenario Generation

```{r}

temperature <- read_excel("data/temperature.xlsx")
humidity <- read_excel("data/relative_humidity.xlsx")

temperature_daily <- temperature %>% 
  group_by(date) %>% 
  summarise(across(starts_with("t_ws"),
                   ~ mean(.x, na.rm = TRUE)))

humidity_daily <- humidity %>% 
  group_by(date) %>% 
  summarise(across(starts_with("rh_ws"),
                   ~ mean(.x, na.rm = TRUE)))

data_all <- load_daily %>% 
  right_join(., temperature_daily) %>% 
  right_join(., humidity_daily)


all_data_ts <- msts((data_all[2:ncol(as.matrix(data_all[ , -1]))]),
                start = c(2005, 1, 1),
                seasonal.periods = c(7, 365.25))
all_data_ts <- all_data_ts[,c("DailyLoad", "t_ws1", "rh_ws1")]

n_for <- 59
train_end  <- c(2009, 365)
test_start <- c(2010, 1)
test_end   <- c(2010, n_for)

scen_train_ts <- window(all_data_ts, end   = train_end)
scen_test_ts  <- window(all_data_ts, start = test_start, end = test_end)


R = cor(scen_train_ts)

horizon <- n_for
nscen <- 10

X = array(0, c(ncol(scen_train_ts), horizon, nscen))

for(i in 1:ncol(scen_train_ts)){
    k = c(2,6)
    fit_SARIMA = auto.arima(scen_train_ts[,i],
                          seasonal = FALSE,
                          lambda = 0,
                          xreg = fourier(scen_train_ts[,i],
                          K = k))
  
    for_SARIMA=forecast(fit_SARIMA,
                      xreg = fourier(scen_train_ts[,i],
                      K = k,
                      h = n_for),
                      h = n_for)   #forecast using the fitted SARIMA
  
    for(t in 1:horizon){
    # we will use the following expression to manually compute sd
    sd=(for_SARIMA$upper[t,1] - for_SARIMA$lower[t,1]) / (2 * qnorm(.5 + for_SARIMA$level[1] / 200))
    
    # Now that I have mean and standard deviation for time t
    # I can draw scenarios using the rnorm() function
    X[i,t,] <- rnorm(nscen,mean=for_SARIMA$mean[t],sd=sd)  
    
    #note this is done in a loop for all the 24 steps we are forecasting 
    #and this loop is inside a loop over all HPP inflows
    
    } # end t loop

  # remove models just to make sure we start from scratch for the next HPP
  # remember we are still inside the HPP loop
  rm(fit_SARIMA, for_SARIMA) 
                      
}

X
```


Now our array/matrix X has all the draws/scenarios but notice they don't have the same correlation we observed in the historical data.
```{r}
#Calculating correlation for s=1
aux <- X[,,1]
cor(t(aux))
```


Let's fix that with Cholesky.

```{r}
U <- chol(R) #that will give upper triangular matrix for Cholesky decomposition
L <- t(U) #to get lower triangular matrix you need to transpose U, that is what the t() function is doing here

#Creating array Y where we will store correlated scenarios
Y <- array(0,c(ncol(scen_train_ts),horizon,nscen)) 

# Need to use another loop structure to make sure spatial correlation among HPP is present in all scenarios
for(s in 1:nscen){ 
  aux <- X[,,s] #creating aux variable simple because X is not a 2x2 matrix, 
                  #but an array of 3 dimension and we cannot do matrix multiplication with arrays
  
  Y[,,s] <- L%*%aux  #recall L is the Cholesky decomposition of our correlation matrix R computed from with historical data

}#end scenario loop


#Calculate correlation again
aux <- Y[,,1]
cor(t(aux))

Y
```


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(forecast)

# years forecasting
test_index <- time(scen_test_ts)                # msts keeps the time index
# make sure length(test_index) == horizon

scenario_df <-
  expand_grid(
    time      = test_index,
    scenario  = seq_len(nscen)
  ) %>% 
  arrange(time, scenario) %>% 
  mutate(
    value = as.vector(t(Y[1, , ]))              # 1 = DailyLoad
  )


fan_df <-
  scenario_df |>
  group_by(time) |>
  summarise(
    p50 = median(value),
    p10 = quantile(value, .10),
    p90 = quantile(value, .90),
    .groups = "drop"
  )

actual_df <- tibble(
  time   = test_index,
  actual = as.numeric(scen_test_ts[, "DailyLoad"])
)


ggplot() +
  geom_line(data = scenario_df,
            aes(time, value, group = scenario),
            colour = "gray60", alpha = 0.3) +
  geom_ribbon(data = fan_df,
              aes(time, ymin = p10, ymax = p90),
              fill = "steelblue", alpha = 0.25) +
  geom_line(data = fan_df,
            aes(time, p50),
            colour = "steelblue", linewidth = 1) +
  geom_line(data = actual_df,
            aes(time, actual),
            colour = "firebrick", linewidth = 1) +
  geom_point(data = actual_df,
             aes(time, actual),
             colour = "firebrick", size = 2) +
  labs(title    = "Daily System Load: scenarios vs actual",
       y        = "MW",
       subtitle = "Gray = each scenario; blue band = 10â€“90 % range; red = actual") +
  theme_classic()

fc <- list(
  model  = "Simulated",
  level  = c(10, 90),
  mean   = ts(fan_df$p50,
              start = start(scen_test_ts),
              frequency = frequency(scen_test_ts)),
  lower  = ts(cbind(fan_df$p10, fan_df$p10),
              start = start(scen_test_ts),
              frequency = frequency(scen_test_ts)),
  upper  = ts(cbind(fan_df$p90, fan_df$p90),
              start = start(scen_test_ts),
              frequency = frequency(scen_test_ts)),
  x      = scen_train_ts[, "DailyLoad"],
  series = "DailyLoad",
  method = "Cholesky-sim"
)

autoplot(fc$mean) +
  autolayer(scen_test_ts[, "DailyLoad"], series = "Actual") +
  labs(title = "Simulated DailyLoad forecasts vs actuals",
       y     = "MW")

accuracy(fc$mean, scen_test_ts[, "DailyLoad"])


```


## Perform on full data for projection


```{r}

temperature <- read_excel("data/temperature.xlsx")
humidity <- read_excel("data/relative_humidity.xlsx")

temperature_daily <- temperature %>% 
  group_by(date) %>% 
  summarise(across(starts_with("t_ws"),
                   ~ mean(.x, na.rm = TRUE)))

humidity_daily <- humidity %>% 
  group_by(date) %>% 
  summarise(across(starts_with("rh_ws"),
                   ~ mean(.x, na.rm = TRUE)))

data_all <- load_daily %>% 
  right_join(., temperature_daily) %>% 
  right_join(., humidity_daily)


all_data_ts <- msts((data_all[2:ncol(as.matrix(data_all[ , -1]))]),
                start = c(2005, 1, 1),
                seasonal.periods = c(7, 365.25))
all_data_ts <- all_data_ts[,c("DailyLoad", "t_ws1", "rh_ws1")]

n_for <- 59
train_end  <- c(2009, 365)
test_start <- c(2010, 1)
test_end   <- c(2010, n_for)


R = cor(all_data_ts)

horizon <- n_for
nscen <- 10

X = array(0, c(ncol(all_data_ts), horizon, nscen))

for(i in 1:ncol(all_data_ts)){
    k = c(2,6)
    fit_SARIMA = auto.arima(all_data_ts[,i],
                          seasonal = FALSE,
                          lambda = 0,
                          xreg = fourier(all_data_ts[,i],
                          K = k))
  
    for_SARIMA=forecast(fit_SARIMA,
                      xreg = fourier(all_data_ts[,i],
                      K = k,
                      h = n_for),
                      h = n_for)   #forecast using the fitted SARIMA
  
    for(t in 1:horizon){
    # we will use the following expression to manually compute sd
    sd=(for_SARIMA$upper[t,1] - for_SARIMA$lower[t,1]) / (2 * qnorm(.5 + for_SARIMA$level[1] / 200))
    
    # Now that I have mean and standard deviation for time t
    # I can draw scenarios using the rnorm() function
    X[i,t,] <- rnorm(nscen,mean=for_SARIMA$mean[t],sd=sd)  
    
    #note this is done in a loop for all the 24 steps we are forecasting 
    #and this loop is inside a loop over all HPP inflows
    
    } # end t loop

  # remove models just to make sure we start from scratch for the next HPP
  # remember we are still inside the HPP loop
  rm(fit_SARIMA, for_SARIMA) 
                      
}

X
```


Now our array/matrix X has all the draws/scenarios but notice they don't have the same correlation we observed in the historical data.
```{r}
#Calculating correlation for s=1
aux <- X[,,1]
cor(t(aux))
```


Let's fix that with Cholesky.

```{r}
U <- chol(R) #that will give upper triangular matrix for Cholesky decomposition
L <- t(U) #to get lower triangular matrix you need to transpose U, that is what the t() function is doing here

#Creating array Y where we will store correlated scenarios
Y <- array(0,c(ncol(all_data_ts),horizon,nscen)) 

# Need to use another loop structure to make sure spatial correlation among HPP is present in all scenarios
for(s in 1:nscen){ 
  aux <- X[,,s] #creating aux variable simple because X is not a 2x2 matrix, 
                  #but an array of 3 dimension and we cannot do matrix multiplication with arrays
  
  Y[,,s] <- L%*%aux  #recall L is the Cholesky decomposition of our correlation matrix R computed from with historical data

}#end scenario loop


#Calculate correlation again
aux <- Y[,,1]
cor(t(aux))

Y
```


```{r}



horizon      <- n_for  # 59
last_train   <- as.Date("2009-12-31")  # end of training sample
test_index   <- seq(last_train + 1,
                    by   = "day",
                    len  = horizon)
scenario_df <-
  expand_grid(
    time     = test_index,
    scenario = seq_len(nscen)
  ) %>%
  arrange(time, scenario) %>%
  mutate(
    value = as.vector(t(Y[1, , ]))  # length = horizon Ã— nscen = 590
  )

fan_df <-
  scenario_df %>%
  group_by(time) %>%
  summarise(
    p50 = median(value),
    p10 = quantile(value, .10),
    p90 = quantile(value, .90),
    .groups = "drop"
  )

actual_df <- tibble(
  time   = test_index,
  actual = window(all_data_ts[,"DailyLoad"],
                  start = c(2010,1), end = c(2010,n_for)) |> as.numeric()
)


```


```{r}
ggplot() +
  geom_line(data = scenario_df,
            aes(time, value, group = interaction(scenario)),
            colour = "gray60", alpha = 0.3) +
  geom_ribbon(data = fan_df,
              aes(time, ymin = p10, ymax = p90),
              fill = "steelblue", alpha = 0.25) +
  geom_line(data = fan_df, aes(time, p50),
            colour = "steelblue", linewidth = 1) +
  geom_line(data = actual_df, aes(time, actual),
            colour = "firebrick", linewidth = 1) +
  labs(title    = "Daily-load scenarios vs actual (Janâ€“Feb 2010)",
       subtitle = "Gray = each scenario; blue band = 10â€“90 % fan; red = actual",
       y = "MW") +
  theme_classic()

```

```{r}
accuracy(fan_df$p50, actual_df$actual)
```


```{r}
forecast_2011_model_8 <- as.numeric(NN_forecast_full$mean)
#Read submission template
template <- read_excel("data/submission_template.xlsx")

#Insert forecast
template$load <- fan_df$p50

#Save as CSV
write.csv(template, "forecasts/forecast_submission_model_9.csv", row.names = FALSE)
```

